#!/usr/bin/env python
# coding: utf-8
from functools import reduce
import glob
import json
import os
import pathlib
import sys
import time

import pandas as pd

import dimcli
from dimcli.utils import chunks_of

print("==\nLogging in..")
# https://digital-science.github.io/dimcli/getting-started.html#authentication
ENDPOINT = "https://app.dimensions.ai"
if 'google.colab' in sys.modules:
  import getpass
  KEY = getpass.getpass(prompt='API Key: ')
  dimcli.login(key=KEY, endpoint=ENDPOINT)
else:
  KEY = ""
  dimcli.login(key=KEY, endpoint=ENDPOINT)
dsl = dimcli.Dsl()

# create list of all files generated by previous text search by date range
complete_filelist = glob.glob("./csv_data/df_exosome_*.csv")
complete_filelist.sort()
processed_filelist = glob.glob("./downloaded_csv_data/raw_*.csv")
processed_filelist.sort()
# echo two filename examples to make sure it works
# print(filelist[0:2])
# x is the result of the previous iteration, y is the current element from iterable
# for example in the master set, x is initially set() and then it becomes the union of the empty set and pd.read_csv(<first element in the filelist>)["id"]
# this is the for loop and deduplication
print(f"Reducing complete set")
complete_set = reduce(lambda x,y: x.union(pd.read_csv(y)["id"]), complete_filelist, set())
print(f"Reducing processed set")
processed_set = reduce(lambda x,y: x + [y.split("raw_")[1].split(".csv")[0]], processed_filelist, [])
# processed_set = reduce(lambda x,y: print(f"{x}-{type(x)}-{y}-{type(y)}-{y.split('raw_')[1].split('.csv')[0]}"), processed_filelist[:10], [])
# processed_set = reduce(lambda x,y: x.union(pd.read_csv(y)["id"]), processed_filelist, set())

failed_filelist = glob.glob("./failed_csv_data/*.csv")
failed_filelist.sort()
# failed_set = reduce(lambda x,y: x.union(pd.read_csv(y)["id"]), failed_filelist, set())
failed_set = reduce(lambda x,y: x + [y.split("data/")[1].split(".csv")[0]], failed_filelist, [])

remaining_set = complete_set - set(processed_set) - set(failed_set)
remaining_set = list(remaining_set)
remaining_set.sort()
# print(json.dumps(list(chunks_of(sorted(complete_set), 3))[:2]))
# exit(8)

print(f"{len(complete_set)} dois in total")
print(f"{len(remaining_set)} more dois remaining")
# exit(0)
while(len(remaining_set) > 0):
    for current_index,remaining_id in enumerate(remaining_set):
        edge_list_filename = f"./citing_cited/edge_list_{remaining_id}.csv"
        raw_data_filename = f"./downloaded_csv_data/raw_{remaining_id}.csv"
        try:
            print(f"{current_index + len(processed_set) + len(failed_set)}/{len(complete_set)}")
            print(f"querying {remaining_id}")
            current_dataframe = dsl.query_iterative(f"""search publications where reference_ids in ["{remaining_id}"] return publications[id+doi+year+reference_ids]""").as_dataframe()
            # current_dataframe = dsl.query_iterative(f"""search publications where id="{remaining_id}" return publications[id+doi+year+reference_ids]""").as_dataframe()
            print(f"stacking {remaining_id}")
            print(f"{current_dataframe.head()}")
            if(current_dataframe.empty):
                with open(f"./failed_csv_data/{remaining_id}.csv", "w") as f:
                    f.write("failed to query {remaining id}\n")
            else:
                cited_citing_dataframe = pd.DataFrame(list(current_dataframe.reference_ids), index=current_dataframe.id).stack().reset_index([0, "id"])
                print(f"writing {remaining_id}")
                cited_citing_dataframe.to_csv(edge_list_filename, columns=["id", 0], header=["citing", "cited"], index=False)
                print(f"updating processed {remaining_id}")
                current_dataframe.to_csv(raw_data_filename, index=False)
        except Exception as e:
            edge_list_path = pathlib.Path(f"{edge_list_filename}")
            raw_data_path = pathlib.Path(f"{raw_data_filename}")
            if(edge_list_path.exists()):
                edge_list_path.unlink()
            if(raw_data_path.exists()):
                raw_data_ppath.unlink()
            print(f"error in executing {remaining_id}")
            print(f"{e}")
    processed_filelist = glob.glob("./downloaded_csv_data/raw_*.csv")
    processed_filelist.sort()
    # processed_set = reduce(lambda x,y: x.union(pd.read_csv(y)["id"]), processed_filelist, set())
    processed_set = reduce(lambda x,y: x + [y.split("raw_")[1].split(".csv")[0]], processed_filelist, [])

    failed_filelist = glob.glob("./failed_csv_data/*.csv")
    failed_filelist.sort()
    # failed_set = reduce(lambda x,y: x.union(pd.read_csv(y)["id"]), failed_filelist, set())
    failed_set = reduce(lambda x,y: x + [y.split("data/")[1].split(".csv")[0]], failed_filelist, [])
    remaining_set = complete_set - set(processed_set) - set(failed_set)
    print(f"{len(remaining_set)} more dois remaining")



print(f"Downloaded all data")
citing_cited_filelist = glob.glob("./citing_cited/edge_list_*.csv")
citing_cited_filelist.sort()
final_network = "./citing_cited.csv"
print(f"Copying files")
with open(final_network, "w") as fw:
    fw.write("citing,cited\n")
    for file_index,citing_cited_file in enumerate(citing_cited_filelist):
        print(f"{file_index}/{len(citing_cited_filelist)}")
        with open(citing_cited_file, "r") as f:
            for line_number,line in enumerate(f):
                if(line_number == 0):
                    continue
                fw.write(line)

# for example here, the x is initally an empty dataframe and then it becomes the row stack of an empty dataframe and then the result of dimension query.as_dataframe()
# chunks_of is identical to the function you had before except it's provided in dimensions utils as you can see above in from dimcli.utils improt chunks_of
#full_dataframe = reduce(lambda x,y: x.append(dsl.query_iterative(f"""search publications where reference_ids in {json.dumps(y)} return publications[id+doi+year+reference_ids]""").as_dataframe(), ignore_index=True, sort=False), chunks_of(sorted(master_set), 3), pd.DataFrame())
# this is a bit trickier but stack takes a list and then an index to turn a dataframe like below
# index list
# -----------
# 0     [a b]
# 1     [a b]
# into a dataframe like below
# index 0 (with a hidden default index)
# ---------
# 0     a 0
#       b 1
# 1     a 2
#       b 3
# so for us the index is the publication id and the list column is the publication reference_ids
# but then the new column is named 0 and pandas creates a default unnamed index or something like that
# it was a lot of trial and error but we want to reset the index so that the rows actually look right
#cited_citing_dataframe = pd.DataFrame(list(full_dataframe.reference_ids), index=full_dataframe.id).stack().reset_index([0, "id"])
#cited_citing_dataframe.to_csv("./citing_cited.csv", columns=["id", 0], header=["citing", "cited"], index=False)
#full_dataframe.to_csv("./full_dataframe.csv", index=False)

